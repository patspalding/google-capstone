{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91daf51e",
   "metadata": {},
   "source": [
    "# Google Data Analytics Capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803ba011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import date, time, datetime, timedelta\n",
    "import os\n",
    "import matplotlib.ticker as mtick\n",
    "import warnings\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ea250e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Fitabase Data 3.12.16-4.11.16/weightLogInfo_merged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kx/qggqdzb11hvcv1t1dl1n24wc0000gn/T/ipykernel_10095/556270191.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"_merged.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m weightLog1 = pd.read_csv(path1 + \"weightLogInfo\" + end).dropna(subset=[\"Id\",\"Date\",\"WeightPounds\",\n\u001b[0m\u001b[1;32m     10\u001b[0m                                                                        \"IsManualReport\"])\n\u001b[1;32m     11\u001b[0m weightLog2 = pd.read_csv(path2 + \"weightLogInfo\" + end).dropna(subset=[\"Id\",\"Date\",\"WeightPounds\",\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Fitabase Data 3.12.16-4.11.16/weightLogInfo_merged.csv'"
     ]
    }
   ],
   "source": [
    "# Read in data from the weightLogInfo and dailyActivity CSVs\n",
    "# The March-April data are labeled with a \"1\", and the April-May data are labeled with a \"2\"\n",
    "\n",
    "path1 = \"Fitabase Data 3.12.16-4.11.16/\"\n",
    "path2 = \"Fitabase Data 4.12.16-5.12.16/\"\n",
    "\n",
    "end = \"_merged.csv\"\n",
    "\n",
    "weightLog1 = pd.read_csv(path1 + \"weightLogInfo\" + end).dropna(subset=[\"Id\",\"Date\",\"WeightPounds\",\n",
    "                                                                       \"IsManualReport\"])\n",
    "weightLog2 = pd.read_csv(path2 + \"weightLogInfo\" + end).dropna(subset=[\"Id\",\"Date\",\"WeightPounds\",\n",
    "                                                                       \"IsManualReport\"])\n",
    "\n",
    "dailyActivity1 = pd.read_csv(path1 + \"dailyActivity\" + end)\n",
    "dailyActivity2 = pd.read_csv(path2 + \"dailyActivity\" + end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad625bd5",
   "metadata": {},
   "source": [
    "To compare the weight and activity data, we will look at the percent change in weight for each user and the trendlines for calories burnt each day, minutes spent sedentary each day, total minutes spent active each day, and minutes spent active each day weighted by the type of activity. \"Very active\" minutes will be given a weight of 5, \"fairly active\" minutes will be given a weight of 1, and \"lightly active\" minutes will be given a weight of 0.2.\n",
    "\n",
    "First, we merge the different datasets into one big dataset that we can more easily manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8870a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes, giving us date, weight in pounds, and if the weight log was manual or not \n",
    "# for all users \n",
    "weightData = pd.concat([weightLog1.loc[:,['Id','Date','WeightPounds','IsManualReport']],\n",
    "                        weightLog2.loc[:,['Id','Date','WeightPounds','IsManualReport']]],ignore_index=True)\n",
    "\n",
    "# Transform the weight log date and time data to Python date objects. This will allow us to match \n",
    "# daily activity data and weight log data\n",
    "weightData['Date'] = pd.to_datetime(weightData['Date'],format=\"%m/%d/%Y %I:%M:%S %p\").dt.date\n",
    "\n",
    "\n",
    "\n",
    "# Repeat dataframe concatenation for daily activity data\n",
    "activityData = pd.concat([dailyActivity1.loc[:,['Id','ActivityDate','VeryActiveMinutes',\n",
    "                                                   'FairlyActiveMinutes','LightlyActiveMinutes',\n",
    "                                                   'SedentaryMinutes','Calories']],\n",
    "                     dailyActivity2.loc[:,['Id','ActivityDate','VeryActiveMinutes',\n",
    "                                                   'FairlyActiveMinutes','LightlyActiveMinutes',\n",
    "                                                   'SedentaryMinutes','Calories']]],ignore_index=True) \n",
    "\n",
    "\n",
    "# Add columns for weighted activity minutes and total activity minutes, as defined above\n",
    "activityData.insert(len(activityData.columns),'TotalActiveMinutes',activityData['LightlyActiveMinutes']\n",
    "                    + activityData['FairlyActiveMinutes'] + activityData['VeryActiveMinutes'])\n",
    "activityData.insert(len(activityData.columns),'WeightedActiveMinutes',\n",
    "                    0.2*activityData['LightlyActiveMinutes'] + 1*activityData['FairlyActiveMinutes'] + \n",
    "                    5*activityData['VeryActiveMinutes'])\n",
    "\n",
    "# As above, transform the actvity date data to Python date objects\n",
    "activityData['ActivityDate'] = pd.to_datetime(activityData['ActivityDate'],format=\"%m/%d/%Y\").dt.date\n",
    "\n",
    "# Rename the column \"ActivityDate\" to \"Date\" for ease of merging\n",
    "activityData = activityData.rename(columns={'ActivityDate':'Date'})\n",
    "\n",
    "# Merge dataframes into single dataframe. From the activity data, only keep ID and date (for ease of merging),\n",
    "# and calories burnt, minutes spent sedentary, total minutes spent active, and weighted minutes spent active, \n",
    "# because these are the data we will be comparing\n",
    "# During this, drop any users who don't have weight log data\n",
    "fullData = activityData[['Id','Date','Calories','SedentaryMinutes','TotalActiveMinutes',\n",
    "                         'WeightedActiveMinutes']].merge(weightData,how='left',on=['Id',\n",
    "                            'Date']).sort_values('Date',\n",
    "                          ignore_index=True).dropna(subset=['WeightPounds'],ignore_index=True)\n",
    "\n",
    "# Transform ID into string for ease of plotting\n",
    "fullData['Id'] = fullData['Id'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of unique user IDs in the weight log data\n",
    "\n",
    "ids = pd.unique(fullData[\"Id\"])\n",
    "\n",
    "print(f\"There are {len(ids)} unique user IDs in the weight log data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666fdc8",
   "metadata": {},
   "source": [
    "Fitbit users can log their weight by inputting it manually, or by using the Fitbit Aria line of smart scales, which syncs the device. Users who log their weight manually are denoted in green, and users who log their weight using a smart scale are denoted in magenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort number of times weight is logged per user ID\n",
    "\n",
    "w_manual = fullData[\"IsManualReport\"] == True\n",
    "\n",
    "LogCounts_manual = fullData[\"Id\"][w_manual].value_counts().to_dict()\n",
    "LogCounts_auto = fullData[\"Id\"][~w_manual].value_counts().to_dict()\n",
    "\n",
    "# Make a histogram based on the dict\n",
    "plt.rcParams['figure.figsize'] = 12,6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(*zip(*LogCounts_manual.items()),color='g',label=\"Manual\")\n",
    "ax.bar(*zip(*LogCounts_auto.items()),color='m',label=\"Automatic\")\n",
    "ax.set_xlabel(\"User ID\")\n",
    "ax.set_ylabel(\"Number of Weight Logs\")\n",
    "ax.legend()\n",
    "ax.set_xticklabels([\"\\n\"*(i%2) + l for i,l in enumerate(list(LogCounts_manual.keys()) + \n",
    "                                                        list(LogCounts_auto.keys()))])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/log_counts.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7670ac",
   "metadata": {},
   "source": [
    "Two users- IDs 6962181067 and 8877689391- log their weights much more frequently than the other 11 users. These are the \"frequent users\". The other group of users are the \"average users\". We will compare how the weight and activity data differ between the two groups during this 2-month period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into \"frequent users\" who log their weights frequently and \"average users\" who log less often\n",
    "\n",
    "LogCounts_total = fullData[\"Id\"].value_counts().to_dict()\n",
    "\n",
    "freq_users = np.array([k for k in LogCounts_total if LogCounts_total[k] > 30])\n",
    "avg_users = np.setdiff1d(np.array(list(LogCounts_total)),freq_users)\n",
    "all_users = np.array(list(LogCounts_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47f161",
   "metadata": {},
   "source": [
    "Then, we set up different Python methods needed for vizualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_change(series):\n",
    "    # Get the percent change from the beginning to end of a pandas Series. Data must be in\n",
    "    # chronological order\n",
    "    \n",
    "    return 100*(series.iloc[-1] - series.iloc[0])/series.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trendline(x,y):\n",
    "    # Get the linear trendline given x and y data\n",
    "    # linregress calculates a linear least-squares regression from two sets of x and y data\n",
    "    result = linregress(x,y)\n",
    "    \n",
    "    return result.slope, result.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_setup(fname,topline):\n",
    "    # Set up a csv file to store data\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        os.remove(fname)\n",
    "    f = open(fname, \"w\")\n",
    "    f.write(topline)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_append(fname,content):\n",
    "    # Set up an \"append file\" function to more easily append files\n",
    "    \n",
    "    f = open(fname, \"a\")\n",
    "    f.write(f\"{content}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_line_plot(title,date,y,ylabel):\n",
    "    # Set up a method to easily make a line plot for a single set of x and y data\n",
    "    # This function assumes that the x data is made up of Python datetime objects\n",
    "        \n",
    "    plt.rcParams['figure.figsize'] = 12,8\n",
    "    plt.rcParams['font.size'] = 12\n",
    "        \n",
    "    plt.title(title)\n",
    "    plt.plot(date,y,'k-')\n",
    "    \n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks([datetime(year=2016, month=3, day=4),datetime(year=2016, month=3, day=11),\n",
    "               datetime(year=2016, month=3, day=18),datetime(year=2016, month=3, day=25),\n",
    "               datetime(year=2016, month=4, day=1),datetime(year=2016, month=4, day=8),\n",
    "               datetime(year=2016, month=4, day=15),datetime(year=2016, month=4, day=22),\n",
    "               datetime(year=2016, month=4, day=29),datetime(year=2016, month=5, day=6)])    \n",
    "    plt.xlim(date.iloc[0],date.iloc[-1])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995da249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_line_subplots(suptitle,subtitles,date,ys,ylabels,trendlines,user_id):\n",
    "    # Set up a function to easily make a plot of four subplots for four sets of y data and a single set of \n",
    "    # x data\n",
    "    # This function assumes that the x data is made up of Python datetime objects\n",
    "    # This function also takes a trendline\n",
    "        \n",
    "    if len(subtitles) != 4 | len(ys) != 4 | len(trendlines) != 4 | len(ylabels) != 4:\n",
    "        return \"Error! 2x2 plot cannot be generated from this data.\"\n",
    "    else:\n",
    "        bool_style = 'b-' if user_id in avg_users else 'r-'\n",
    "            \n",
    "        plt.rcParams['font.size'] = 12\n",
    "        plt.rcParams['figure.figsize'] = 16,16\n",
    "\n",
    "        fig, ax = plt.subplots(2, 2,sharex=True)\n",
    "        \n",
    "        ax[0,0].set_title(subtitles[0])\n",
    "        ax[0,0].plot(date,ys[0],'k-')\n",
    "        ax[0,0].plot(date,trendlines[0],bool_style)\n",
    "        ax[0,0].set_ylabel(ylabels[0])\n",
    "        \n",
    "        ax[0,1].set_title(subtitles[1])\n",
    "        ax[0,1].plot(date,ys[1],'k-')\n",
    "        ax[0,1].plot(date,trendlines[1],bool_style)\n",
    "        ax[0,1].set_ylabel(ylabels[1])\n",
    "\n",
    "        ax[1,0].set_title(subtitles[2])\n",
    "        ax[1,0].plot(date,ys[2],'k-')\n",
    "        ax[1,0].plot(date,trendlines[2],bool_style)\n",
    "        ax[1,0].set_ylabel(ylabels[2])\n",
    "\n",
    "        ax[1,1].set_title(subtitles[3])\n",
    "        ax[1,1].plot(date,ys[3],'k-')\n",
    "        ax[1,1].plot(date,trendlines[3],bool_style)\n",
    "        ax[1,1].set_ylabel(ylabels[3])\n",
    "\n",
    "        plt.suptitle(suptitle,fontsize=14)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.xticks([datetime(year=2016, month=3, day=4),datetime(year=2016, month=3, day=11),\n",
    "                   datetime(year=2016, month=3, day=18),datetime(year=2016, month=3, day=25),\n",
    "                   datetime(year=2016, month=4, day=1),datetime(year=2016, month=4, day=8),\n",
    "                   datetime(year=2016, month=4, day=15),datetime(year=2016, month=4, day=22),\n",
    "                   datetime(year=2016, month=4, day=29),datetime(year=2016, month=5, day=6)])    \n",
    "        plt.xlim(date.iloc[0],date.iloc[-1])\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ab604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bar_from_csv(fname,title):\n",
    "    data = pd.read_csv(fname,dtype={\"ID\":str}).sort_values(\"Percent change\")\n",
    "\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.rcParams['figure.figsize'] = 12,8\n",
    "    \n",
    "    bool_colors = np.where(np.array([i in avg_users for i in data[\"ID\"]]) == True,'b','r')\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(title)\n",
    "    ax.bar(data[\"ID\"],data[\"Percent change\"],color=bool_colors)\n",
    "    ax.set_xticklabels([\"\\n\"*(i%2) + l for i,l in enumerate(data[\"ID\"])])\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    plt.xlabel(\"User ID\")\n",
    "    plt.ylabel(\"Percent change\")\n",
    "    plt.axhline(0,c='k',ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(id_list,data):\n",
    "    # This function generates all the plots we want from the weight and activity data\n",
    "    # with the exception of the percent change graphs\n",
    "        \n",
    "    # Set up dictionary to store trendline data over the loop\n",
    "    trendlines = {'calorie':{'m':[],'b':[]},'sedentary':{'m':[],'b':[]},'total_activity':{'m':[],'b':[]},\n",
    "                 'weighted_activity':{'m':[],'b':[]}}\n",
    "        \n",
    "    # Select the indices where the given ID matches the user ID in the data\n",
    "    for i in id_list:\n",
    "        w_userid = data['Id'] == i\n",
    "        userData = data[w_userid]\n",
    "\n",
    "        # Append the file if there is more than one non-NaN data point- percent change doesn't make sense as a\n",
    "        # metric if there is only one data point\n",
    "        if len(userData['WeightPounds'].dropna()) > 1:\n",
    "            file_append(\"generated_csvs/weight_change.csv\",f\"{i},\" + \n",
    "                        f\"{get_percent_change(userData['WeightPounds'].dropna())}\")\n",
    "            \n",
    "            # Make a line plot of the weight over time\n",
    "            make_line_plot(f\"Weight over Time for ID {i}\",userData.dropna(subset='WeightPounds')['Date'],\n",
    "                           userData['WeightPounds'].dropna(),\"Weight (Lbs)\")\n",
    "            plt.savefig(f\"images/{i}_weight.png\")\n",
    "            plt.show()\n",
    "            \n",
    "        # Plot trendlines if there is more than one non-NaN data point- trendlines don't make sense\n",
    "        # if there is only one data point\n",
    "        if ((len(userData['Calories'].dropna()) > 1) and (len(userData['SedentaryMinutes'].dropna()) > 1) \n",
    "            and (len(userData['TotalActiveMinutes'].dropna()) > 1) and \n",
    "            len(userData['WeightedActiveMinutes'].dropna()) > 1):\n",
    "            # Drop data points where calories burnt is small- this is an implausible measurement, so we assume\n",
    "            # the activity data associated with these measurements are in some way flawed\n",
    "            w_zero_cal = userData['Calories'] < 100\n",
    "            userData = userData.drop(userData[w_zero_cal].index)\n",
    "\n",
    "            # Create plot of 2x2 subplots daily calories burnt, daily minutes spent sedentary, daily minutes\n",
    "            # spent in any activity, and daily minutes spent active weighted by activity category\n",
    "\n",
    "            dateSeconds = np.array([(datetime(*i.timetuple()[:6]) - datetime(1970, 1, 1)) / timedelta(seconds=1) \n",
    "                        for i in userData['Date']]).astype('float')\n",
    "\n",
    "            # Get trendlines for each object, and store in dict for later use; also store trendline in a list\n",
    "            # for plotting\n",
    "            trendlines_list = []\n",
    "\n",
    "            m,b = get_trendline(dateSeconds,userData['Calories'])\n",
    "            trendlines['calorie']['m'].append(m)\n",
    "            trendlines['calorie']['b'].append(b)\n",
    "            trendlines_list.append(m*dateSeconds+b)\n",
    "\n",
    "            m,b = get_trendline(dateSeconds,userData['SedentaryMinutes'])\n",
    "            trendlines['sedentary']['m'].append(m)\n",
    "            trendlines['sedentary']['b'].append(b)\n",
    "            trendlines_list.append(m*dateSeconds+b)\n",
    "\n",
    "            m,b = get_trendline(dateSeconds,userData['TotalActiveMinutes'])\n",
    "            trendlines['total_activity']['m'].append(m)\n",
    "            trendlines['total_activity']['b'].append(b)\n",
    "            trendlines_list.append(m*dateSeconds+b)\n",
    "\n",
    "            m,b = get_trendline(dateSeconds,userData['WeightedActiveMinutes'])\n",
    "            trendlines['weighted_activity']['m'].append(m)\n",
    "            trendlines['weighted_activity']['b'].append(b)\n",
    "            trendlines_list.append(m*dateSeconds+b)\n",
    "\n",
    "            # Send everything to subplot method\n",
    "            make_line_subplots(f\"Activity Data for ID {i}\",\n",
    "                               [\"Daily Calories Burnt\",\"Daily Sedentary Minutes\",\n",
    "                                                            \"Daily Total Active Minutes\",\n",
    "                                                            \"Daily Weighted Active Minutes\"],\n",
    "                               userData['Date'],\n",
    "                               [userData['Calories'],userData['SedentaryMinutes'],userData['TotalActiveMinutes'],\n",
    "                                userData['WeightedActiveMinutes']],\n",
    "                               [\"Calories Burnt\",\"Minutes Spent Sedentary\",\n",
    "                                                \"Minutes Spent Active (Total)\",\"Minutes Spent Active (Weighted)\"],\n",
    "                               trendlines_list,i)\n",
    "            plt.savefig(f\"images/{i}_activity.png\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be823b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_setup(\"generated_csvs/weight_change.csv\",\"ID,Percent change\\n\")\n",
    "\n",
    "plot_data(avg_users,fullData)\n",
    "plot_data(freq_users,fullData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849765ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make bar plot to show percent change in weight over the two months for all users\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "make_bar_from_csv(\"generated_csvs/weight_change.csv\",\"Change in Weight\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"images/weight_percent_change.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
